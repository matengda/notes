　在计算机历史的早期，美国为代表的英语系国家主导了整个计算机行业，26个英文字母组成了多样的英语单词、语句、文章。

因此，最早的字符编码规范是**ASCII码，一种8位即1个字节的编码规范**，它可以涵盖整个英语系的编码需要。

- 01000001表示大写字母**A**，有时我们会“偷懒"的用 **65** 这个十进制来表示A在`ASCII`中的编码。8个比特位，最多可表示2的8次方(255)个字符



所有的东西，不管是英文、中文还是符号等等，最终存储在磁盘上都是01010101这类东西。在计算机内部，读取和存储数据归根结底，处理的都是0和1组成的比特流

后来，计算机得到普及，中文、日文、韩文等等国家的文字需要在计算机内表示，ASCII的255位远远不够，于是标准组织制定出了叫做**UNICODE的万国码**，

它规定**任何一个字符（不管哪国的）至少以2个字节表示，可以更多**

其中，英文字母就是用2个字节，而汉字是3个字节。这个编码虽然很好，满足了所有人的要求，但是它不兼容`ASCII`，同时还占用较多的空间和内存，不便于**网络传输**和**文件存储**。因为，在计算机世界更多的字符是英文字母，明明可以1个字节就能够表示，非要用2个。

于是`UTF-8`编码应运而生，它规定

- **英文字母系列用1个字节表示，**
- **欧洲文字通常用两个字节表示，**
- **(CJK 统一表意符号 (CJK Unified Ideographs):4E00-9FBF)\**汉字等东亚文字\**用3个字节表示**等等。